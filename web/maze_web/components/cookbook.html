<!-- Cookbook Section Component -->
<section class="cookbook-section" id="cookbook">
    <div class="cookbook-container">
        <h2 class="cookbook-title">üìö Cookbook</h2>
        <p class="cookbook-subtitle">Learn by example - from simple workflows to complex applications</p>
        
        <!-- Example 1: Simple Workflow with Typewriter Animation -->
        <div class="cookbook-example featured-example">
            <div class="example-header">
                <h3>üöÄ Quick Start: Simple Two-Task Workflow</h3>
                <span class="example-badge">Interactive</span>
            </div>
            <p class="example-description">
                A minimal example showing how to create a workflow with two sequential tasks. 
                Each task receives input, processes it with a timestamp, and passes the result to the next task.
            </p>
            
            <div class="example-structure">
                <div class="structure-title">Workflow Structure:</div>
                <div class="structure-diagram">
                    <span class="structure-node">Task 1</span>
                    <span class="structure-arrow">‚Üí</span>
                    <span class="structure-node">Task 2</span>
                </div>
            </div>
            
            <div class="code-container">
                <div class="code-header">
                    <div class="code-title">
                        <span class="code-icon">üìÑ</span>
                        <span>simple_workflow.py</span>
                    </div>
                    <button class="code-replay-btn" onclick="replayTypewriter(0)">
                        <span class="replay-icon">‚Üª</span> Replay
                    </button>
                </div>
                <pre class="code-content"><code id="typewriter-code-0" class="language-python"></code></pre>
            </div>
            
            <div class="example-footer">
                <span class="example-feature">‚ö° Sequential Execution</span>
                <span class="example-feature">üîó Auto Dependencies</span>
                <span class="example-feature">üìä Resource Management</span>
            </div>
        </div>

        <!-- Example 2: Financial Risk Assessment -->
        <div class="cookbook-example">
            <div class="example-header">
                <h3>üí∞ Financial Risk Assessment Workflow</h3>
                <span class="example-badge advanced">Advanced</span>
            </div>
            <p class="example-description">
                Multi-dimensional risk assessment combining LLM analysis with parallel Monte Carlo simulations. 
                This example demonstrates heterogeneous resource allocation with CPU-intensive risk calculations.
            </p>
            
            <div class="example-structure">
                <div class="structure-title">Workflow Structure:</div>
                <div class="structure-diagram complex">
                    <div class="structure-row">
                        <span class="structure-node">LLM Analysis</span>
                    </div>
                    <div class="structure-row parallel">
                        <span class="structure-node small">Market Risk</span>
                        <span class="structure-node small">Credit Risk</span>
                        <span class="structure-node small">Liquidity Risk</span>
                    </div>
                    <div class="structure-row">
                        <span class="structure-node">Report Generation</span>
                    </div>
                </div>
            </div>
            
            <div class="code-container">
                <div class="code-header">
                    <div class="code-title">
                        <span class="code-icon">üìÑ</span>
                        <span>financial_risk_workflow.py</span>
                    </div>
                    <button class="code-expand-btn" onclick="toggleCode(1)">
                        <span class="expand-text">Expand</span>
                        <span class="expand-icon">‚ñº</span>
                    </button>
                </div>
                <pre class="code-content collapsed" id="code-block-1"><code class="language-python">"""
Financial Risk Assessment Workflow - Using LangGraph

Business Scenario:
Multi-dimensional risk assessment for investment portfolios, including:
1. Market Risk Analysis (VaR calculation)
2. Credit Risk Assessment (Monte Carlo simulation)
3. Liquidity Risk Analysis (Stress testing)

Workflow:
1. LLM analyzes portfolio and extracts key parameters
2. Execute three compute-intensive risk assessment tools in parallel
3. Aggregate results and generate risk report
"""

import os
from typing import TypedDict, Annotated, List
from operator import add
import numpy as np
from langchain_community.chat_models.tongyi import ChatTongyi
from langgraph.graph import StateGraph, START, END
from maze import LanggraphClient

client = LanggraphClient("localhost:8000")

# Configure environment variables
os.environ["DASHSCOPE_API_KEY"] = os.getenv("DASHSCOPE_API_KEY")
model = ChatTongyi(streaming=True)


# Define state
class GraphState(TypedDict):
    """Workflow state"""
    portfolio_description: str  # Portfolio description
    portfolio_params: dict  # Parameters extracted by LLM
    market_risk_result: str  # Market risk result
    credit_risk_result: str  # Credit risk result
    liquidity_risk_result: str  # Liquidity risk result
    risk_reports: Annotated[List[str], add]  # Accumulated risk reports

 
def llm_analysis_node(state: GraphState):
    """Use LLM to analyze portfolio description and extract key parameters"""
    print("\n=== LLM Analysis Node ===")
    portfolio_desc = state["portfolio_description"]
    
    prompt = f"""
    Please analyze the following portfolio description and extract key risk assessment parameters:
    
    Portfolio Description:
    {portfolio_desc}
    
    Please provide the following information:
    1. Portfolio initial value (in 10,000 USD)
    2. Expected return rate (%)
    3. Volatility (%)
    4. Confidence level (90%, 95%, or 99%)
    5. Time horizon (days)
    
    Return in the format:
    initial_value: number
    return_rate: number
    volatility: number
    confidence_level: number
    time_horizon: number
    """
    
    response = model.invoke(prompt)
    llm_response = response.content
    
    # Parse LLM response (simplified version)
    params = {
        "initial_value": 1000000,  # $1M
        "return_rate": 0.08,       # 8% annual return
        "volatility": 0.15,        # 15% volatility
        "confidence_level": 0.95,  # 95% confidence
        "time_horizon": 252        # 1 trading year
    }
    
    print(f"LLM Response: {llm_response[:200]}...")
    print(f"Extracted Parameters: {params}")
    
    return {"portfolio_params": params}


@client.tool(
    cpu=4,
    memory=2048,
    description="Calculate Market Risk using Value at Risk (VaR) method"
)
def market_risk_assessment(state: GraphState):
    """Compute-intensive: Calculate VaR using Monte Carlo simulation"""
    print("\n=== Market Risk Assessment Tool ===")
    params = state["portfolio_params"]
    
    # Monte Carlo VaR calculation
    np.random.seed(42)
    n_simulations = 100000
    
    daily_return = params["return_rate"] / 252
    daily_vol = params["volatility"] / np.sqrt(252)
    
    random_returns = np.random.normal(
        daily_return, 
        daily_vol, 
        (n_simulations, params["time_horizon"])
    )
    
    portfolio_values = params["initial_value"] * np.exp(
        np.cumsum(random_returns, axis=1)
    )
    
    final_values = portfolio_values[:, -1]
    var = np.percentile(final_values, (1 - params["confidence_level"]) * 100)
    var_amount = params["initial_value"] - var
    
    result = f"""
    Market Risk (VaR) Assessment:
    - Confidence Level: {params['confidence_level']*100}%
    - Time Horizon: {params['time_horizon']} days
    - Value at Risk: ${var_amount:,.2f}
    - Maximum Expected Loss: {(var_amount/params['initial_value'])*100:.2f}%
    """
    
    print(result)
    return {
        "market_risk_result": result,
        "risk_reports": [result]
    }


# Create workflow
workflow = StateGraph(GraphState)

# Add nodes
workflow.add_node("llm_analysis", llm_analysis_node)
workflow.add_node("market_risk", market_risk_assessment)

# Define edges
workflow.add_edge(START, "llm_analysis")
workflow.add_edge("llm_analysis", "market_risk")
workflow.add_edge("market_risk", END)

# Compile and run
app = workflow.compile()

if __name__ == "__main__":
    print("üöÄ Starting Financial Risk Assessment Workflow...")
    
    initial_state = {
        "portfolio_description": """
        Technology-focused investment portfolio with $1M initial capital.
        Primarily invested in tech stocks with expected 8% annual return.
        Historical volatility around 15%. Need 95% confidence risk assessment
        for 1-year holding period.
        """,
        "risk_reports": []
    }
    
    final_state = app.invoke(initial_state)
    
    print("\n" + "="*60)
    print("üìä Risk Assessment Complete!")
    print("="*60)
    for report in final_state.get("risk_reports", []):
        print(report)
</code></pre>
            </div>
            
            <div class="example-footer">
                <span class="example-feature">üß† LLM + Monte Carlo</span>
                <span class="example-feature">‚ö° Parallel Execution</span>
                <span class="example-feature">üìà Financial Analysis</span>
            </div>
        </div>

        <!-- Example 3: Multimodal Content Generator -->
        <div class="cookbook-example">
            <div class="example-header">
                <h3>üé® Multimodal Creative Content Generator</h3>
                <span class="example-badge advanced">Advanced</span>
            </div>
            <p class="example-description">
                Generate text, image, and audio content in parallel from a single description. 
                Demonstrates GPU/CPU heterogeneous scheduling with HuggingFace models (GPT-2, Stable Diffusion, Bark TTS).
            </p>
            
            <div class="example-structure">
                <div class="structure-title">Workflow Structure:</div>
                <div class="structure-diagram complex">
                    <div class="structure-row">
                        <span class="structure-node">Input Processing</span>
                    </div>
                    <div class="structure-row parallel">
                        <span class="structure-node small">Text Gen<br/><span class="resource-tag">CPU</span></span>
                        <span class="structure-node small">Image Gen<br/><span class="resource-tag gpu">GPU</span></span>
                        <span class="structure-node small">Audio Gen<br/><span class="resource-tag">CPU</span></span>
                    </div>
                    <div class="structure-row">
                        <span class="structure-node">HTML Report</span>
                    </div>
                </div>
            </div>
            
            <div class="code-container">
                <div class="code-header">
                    <div class="code-title">
                        <span class="code-icon">üìÑ</span>
                        <span>multimodal_generator.py</span>
                    </div>
                    <button class="code-expand-btn" onclick="toggleCode(2)">
                        <span class="expand-text">Expand</span>
                        <span class="expand-icon">‚ñº</span>
                    </button>
                </div>
                <pre class="code-content collapsed" id="code-block-2"><code class="language-python">"""
Multimodal Creative Content Generator

Workflow Structure:
    Input Description (Task A)
         ‚Üì
    [Task B: Text Generation (CPU, 4GB)]
    [Task C: Image Generation (GPU, 8GB)]  ‚Üê Parallel
    [Task D: Audio Generation (CPU, 4GB)]
         ‚Üì
    Task E: Aggregation & Report (CPU, 512MB)

Models Used:
- Text Generation: gpt2-large (774M)
- Image Generation: stable-diffusion-v1-5 (4GB VRAM)
- Audio Generation: bark (text-to-speech)
"""

from maze.client.maze.client import MaClient
from maze.client.maze.decorator import task
import os

OUTPUT_DIR = r"./outputs/multimodal"
os.makedirs(OUTPUT_DIR, exist_ok=True)


# Task A: Preprocessing and prompt enhancement
@task(
    inputs=["user_description"],
    outputs=["enhanced_text_prompt", "enhanced_image_prompt", "audio_text"],
    resources={"cpu": 1, "cpu_mem": 512, "gpu": 0, "gpu_mem": 0}
)
def preprocess_and_enhance(params):
    """
    Preprocess user input and generate optimized prompts for different modalities
    """
    description = params.get("user_description")
    
    print("=" * 60)
    print("Task A: Preprocessing Input")
    print("=" * 60)
    print(f"User Description: {description}\n")
    
    # Generate prompts for different modalities
    text_prompt = f"Create a creative story about: {description}"
    
    image_prompt = f"high quality, detailed illustration of {description}, " \
                  f"digital art, concept art, trending on artstation"
    
    audio_text = f"Welcome to our creative content about {description}. " \
                f"Let me tell you an interesting story."
    
    print(f"‚úì Text Prompt: {text_prompt[:100]}...")
    print(f"‚úì Image Prompt: {image_prompt[:100]}...")
    print(f"‚úì Audio Text: {audio_text[:100]}...")
    
    return {
        "enhanced_text_prompt": text_prompt,
        "enhanced_image_prompt": image_prompt,
        "audio_text": audio_text
    }


# Task B: Text Generation (CPU-intensive)
@task(
    inputs=["text_prompt"],
    outputs=["generated_text", "text_file_path"],
    resources={"cpu": 4, "cpu_mem": 4096, "gpu": 0, "gpu_mem": 0}
)
def generate_text(params):
    """Generate creative text using GPT-2 Large"""
    from transformers import GPT2LMHeadModel, GPT2Tokenizer
    import torch
    
    print("\n" + "=" * 60)
    print("Task B: Text Generation (CPU)")
    print("=" * 60)
    
    prompt = params.get("text_prompt")
    
    # Load model
    model_name = "gpt2-large"
    print(f"Loading {model_name}...")
    tokenizer = GPT2Tokenizer.from_pretrained(model_name)
    model = GPT2LMHeadModel.from_pretrained(model_name)
    model.eval()
    
    # Generate text
    print(f"Generating text from prompt: {prompt[:50]}...")
    inputs = tokenizer.encode(prompt, return_tensors="pt")
    
    with torch.no_grad():
        outputs = model.generate(
            inputs,
            max_length=300,
            num_return_sequences=1,
            temperature=0.8,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # Save to file
    text_path = os.path.join(OUTPUT_DIR, "generated_story.txt")
    with open(text_path, "w", encoding="utf-8") as f:
        f.write(generated)
    
    print(f"‚úì Generated {len(generated)} characters")
    print(f"‚úì Saved to: {text_path}")
    print(f"Preview: {generated[:200]}...")
    
    return {
        "generated_text": generated,
        "text_file_path": text_path
    }


# Task C: Image Generation (GPU-intensive)
@task(
    inputs=["image_prompt"],
    outputs=["image_path"],
    resources={"cpu": 2, "cpu_mem": 2048, "gpu": 1, "gpu_mem": 8192}
)
def generate_image(params):
    """Generate image using Stable Diffusion"""
    from diffusers import StableDiffusionPipeline
    import torch
    
    print("\n" + "=" * 60)
    print("Task C: Image Generation (GPU)")
    print("=" * 60)
    
    prompt = params.get("image_prompt")
    
    # Load model
    model_id = "runwayml/stable-diffusion-v1-5"
    print(f"Loading {model_id}...")
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    pipe = StableDiffusionPipeline.from_pretrained(
        model_id,
        torch_dtype=torch.float16 if device == "cuda" else torch.float32
    )
    pipe = pipe.to(device)
    
    # Generate image
    print(f"Generating image on {device}...")
    print(f"Prompt: {prompt}")
    
    image = pipe(
        prompt,
        num_inference_steps=50,
        guidance_scale=7.5
    ).images[0]
    
    # Save
    image_path = os.path.join(OUTPUT_DIR, "generated_image.png")
    image.save(image_path)
    
    print(f"‚úì Image generated: {image.size}")
    print(f"‚úì Saved to: {image_path}")
    
    return {"image_path": image_path}


# Create workflow
def main():
    print("\n" + "üé®" * 30)
    print("Multimodal Creative Content Generator")
    print("üé®" * 30 + "\n")
    
    # Connect to Maze server
    client = MaClient("http://localhost:8000")
    workflow = client.create_workflow()
    
    # Task A: Preprocessing
    task_a = workflow.add_task(
        preprocess_and_enhance,
        inputs={"user_description": "a magical forest with glowing mushrooms"}
    )
    
    # Tasks B, C, D: Parallel generation
    task_b = workflow.add_task(
        generate_text,
        inputs={"text_prompt": task_a.outputs["enhanced_text_prompt"]}
    )
    
    task_c = workflow.add_task(
        generate_image,
        inputs={"image_prompt": task_a.outputs["enhanced_image_prompt"]}
    )
    
    # Run workflow
    print("\nüöÄ Starting workflow execution...")
    workflow.run()
    
    # Get results
    for message in workflow.get_results():
        if message.get("type") == "finish_workflow":
            print("\n‚úÖ Workflow completed successfully!")
            break

if __name__ == "__main__":
    main()
</code></pre>
            </div>
            
            <div class="example-footer">
                <span class="example-feature">üé® 3 Modalities</span>
                <span class="example-feature">üñ•Ô∏è GPU + CPU</span>
                <span class="example-feature">ü§ñ HuggingFace Models</span>
            </div>
        </div>
    </div>
</section>



