"""
å¤šæ¨¡æ€åˆ›æ„å†…å®¹ç”Ÿæˆå™¨
===================

åŠŸèƒ½ï¼šä»ä¸€ä¸ªæ–‡æœ¬æè¿°ï¼Œå¹¶è¡Œç”Ÿæˆæ–‡æœ¬æ•…äº‹ã€å›¾åƒå’ŒéŸ³é¢‘ä¸‰ç§æ¨¡æ€çš„å†…å®¹

å·¥ä½œæµç»“æ„ï¼š
    è¾“å…¥æè¿° (Task A)
         â†“
    [Task B: æ–‡æœ¬ç”Ÿæˆ (CPU, 4GB)]
    [Task C: å›¾åƒç”Ÿæˆ (GPU, 8GB)]  â† å¹¶è¡Œæ‰§è¡Œ
    [Task D: éŸ³é¢‘ç”Ÿæˆ (CPU, 4GB)]
         â†“
    Task E: æ±‡æ€»å±•ç¤º (CPU, 512MB)

ä½¿ç”¨æ¨¡å‹ï¼š
- æ–‡æœ¬ç”Ÿæˆ: gpt2-large (774M)
- å›¾åƒç”Ÿæˆ: stable-diffusion-v1-5 (4GB VRAM)
- éŸ³é¢‘ç”Ÿæˆ: bark (text-to-speech)

è¿è¡Œå‰å‡†å¤‡ï¼š
1. ç¡®ä¿ Maze æœåŠ¡å™¨å·²å¯åŠ¨
2. å®‰è£…ä¾èµ–: pip install transformers diffusers torch accelerate scipy bark
3. é¦–æ¬¡è¿è¡Œä¼šä¸‹è½½æ¨¡å‹ï¼Œéœ€è¦ä¸€äº›æ—¶é—´

è¿è¡Œæ–¹å¼ï¼š
python example/multimodal_content_generator.py
"""

from maze import MaClient, task
import os

# è¾“å‡ºç›®å½•é…ç½®ï¼ˆä½¿ç”¨ç»å¯¹è·¯å¾„ï¼‰
OUTPUT_DIR = r"E:\PythonProject\maze\Maze\examples\multimodel_create_workflow\outputs"
os.makedirs(OUTPUT_DIR, exist_ok=True)


# ============================================================================
# Task A: æ–‡æœ¬é¢„å¤„ç†å’Œæç¤ºè¯å¢å¼º
# ============================================================================
@task(
    inputs=["user_description"],
    outputs=["enhanced_text_prompt", "enhanced_image_prompt", "audio_text"],
    resources={"cpu": 1, "cpu_mem": 512, "gpu": 0, "gpu_mem": 0}
)
def preprocess_and_enhance(params):
    """
    é¢„å¤„ç†ç”¨æˆ·è¾“å…¥ï¼Œä¸ºä¸åŒæ¨¡æ€ç”Ÿæˆä¼˜åŒ–çš„æç¤ºè¯
    
    è¾“å…¥: ç”¨æˆ·çš„ç®€çŸ­æè¿°
    è¾“å‡º: 
        - enhanced_text_prompt: ç”¨äºæ–‡æœ¬ç”Ÿæˆçš„è¯¦ç»†æç¤º
        - enhanced_image_prompt: ç”¨äºå›¾åƒç”Ÿæˆçš„ä¼˜åŒ–æç¤º
        - audio_text: ç”¨äºè¯­éŸ³åˆæˆçš„æ–‡æœ¬
    """
    description = params.get("user_description")
    
    print(f"[Task A] Processing user input: {description}")
    
    # Enhance prompts for text generation
    text_prompt = f"Write a creative short story (3-4 paragraphs) about: {description}. Make it engaging and vivid."
    
    # Optimize prompts for image generation (Stable Diffusion friendly)
    image_prompt = f"{description}, highly detailed, digital art, trending on artstation, vibrant colors, 8k uhd"
    
    # Prepare concise text for audio synthesis
    audio_text = f"This is a story about {description}."
    
    print(f"[Task A] âœ“ Prompt enhancement completed")
    print(f"  - Text prompt: {text_prompt[:50]}...")
    print(f"  - Image prompt: {image_prompt[:50]}...")
    print(f"  - Audio text: {audio_text}")
    
    return {
        "enhanced_text_prompt": text_prompt,
        "enhanced_image_prompt": image_prompt,
        "audio_text": audio_text
    }


# ============================================================================
# Task B: æ–‡æœ¬æ•…äº‹ç”Ÿæˆ (CPU)
# ============================================================================
@task(
    inputs=["text_prompt"],
    outputs=["generated_story", "story_file_path"],
    resources={"cpu": 4, "cpu_mem": 4096, "gpu": 0, "gpu_mem": 0}
)
def generate_story(params):
    """
    ä½¿ç”¨ GPT-2 Large ç”Ÿæˆåˆ›æ„æ•…äº‹
    
    æ¨¡å‹: gpt2-large (774M parameters)
    èµ„æº: CPU only, 4 cores, 4GB RAM
    """
    prompt = params.get("text_prompt")
    
    print(f"[Task B] Starting text story generation...")
    print(f"[Task B] Loading GPT-2 Large model...")
    
    from transformers import GPT2LMHeadModel, GPT2Tokenizer
    import torch
    
    # Load model
    model_name = "gpt2-large"
    tokenizer = GPT2Tokenizer.from_pretrained(model_name)
    model = GPT2LMHeadModel.from_pretrained(model_name)
    model.eval()
    
    print(f"[Task B] Model loaded, starting generation...")
    
    # ç¼–ç è¾“å…¥
    inputs = tokenizer.encode(prompt, return_tensors="pt")
    
    # ç”Ÿæˆæ–‡æœ¬
    with torch.no_grad():
        outputs = model.generate(
            inputs,
            max_length=300,
            num_return_sequences=1,
            temperature=0.8,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # è§£ç è¾“å‡º
    story = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    story_file = os.path.join(OUTPUT_DIR, "generated_story.txt")
    with open(story_file, "w", encoding="utf-8") as f:
        f.write(story)
    
    print(f"[Task B] âœ“ Story generation completed!")
    print(f"[Task B]   Story length: {len(story)} characters")
    print(f"[Task B]   Saved to: {story_file}")
    print(f"[Task B]   Preview: {story[:150]}...")
    
    return {
        "generated_story": story,
        "story_file_path": story_file
    }


# ============================================================================
# Task C: å›¾åƒç”Ÿæˆ (GPU)
# ============================================================================
@task(
    inputs=["image_prompt"],
    outputs=["image_file_path", "image_info"],
    resources={"cpu": 2, "cpu_mem": 2048, "gpu": 1, "gpu_mem": 8192}
)
def generate_image(params):
    """
    ä½¿ç”¨ Stable Diffusion ç”Ÿæˆå›¾åƒ
    
    æ¨¡å‹: stable-diffusion-v1-5
    èµ„æº: 1 GPU (4090), 8GB VRAM
    """
    prompt = params.get("image_prompt")
    
    print(f"[Task C] Starting image generation...")
    print(f"[Task C] Prompt: {prompt}")
    print(f"[Task C] Loading Stable Diffusion model...")
    
    from diffusers import StableDiffusionPipeline
    import torch
    
    # åŠ è½½æ¨¡å‹åˆ° GPU
    model_id = "runwayml/stable-diffusion-v1-5"
    pipe = StableDiffusionPipeline.from_pretrained(
        model_id,
        torch_dtype=torch.float16,
        safety_checker=None  # ç¦ç”¨å®‰å…¨æ£€æŸ¥å™¨ä»¥èŠ‚çœå†…å­˜
    )
    pipe = pipe.to("cuda")
    
    print(f"[Task C] Model loaded, starting image generation...")
    
    # ç”Ÿæˆå›¾åƒ
    with torch.no_grad():
        image = pipe(
            prompt,
            num_inference_steps=50,
            guidance_scale=7.5,
            height=512,
            width=512
        ).images[0]
    
    # ä¿å­˜å›¾åƒ
    image_file = os.path.join(OUTPUT_DIR, "generated_image.png")
    image.save(image_file)
    
    # è·å–å›¾åƒä¿¡æ¯
    image_info = f"512x512, Stable Diffusion v1.5, 50 steps"
    
    print(f"[Task C] âœ“ Image generation completed!")
    print(f"[Task C]   Size: 512x512")
    print(f"[Task C]   Saved to: {image_file}")
    
    # æ¸…ç† GPU å†…å­˜
    del pipe
    torch.cuda.empty_cache()
    
    return {
        "image_file_path": image_file,
        "image_info": image_info
    }


# ============================================================================
# Task D: éŸ³é¢‘ç”Ÿæˆ (CPU)
# ============================================================================
@task(
    inputs=["audio_text"],
    outputs=["audio_file_path", "audio_duration"],
    resources={"cpu": 4, "cpu_mem": 4096, "gpu": 0, "gpu_mem": 0}
)
def generate_audio(params):
    """
    ä½¿ç”¨ Bark å°†æ–‡æœ¬è½¬æ¢ä¸ºè¯­éŸ³
    
    æ¨¡å‹: suno/bark-small
    èµ„æº: CPU only, 4 cores, 4GB RAM
    """
    text = params.get("audio_text")
    
    print(f"[Task D] Starting audio generation...")
    print(f"[Task D] Text: {text}")
    print(f"[Task D] Loading Bark TTS model...")
    
    from transformers import AutoProcessor, BarkModel
    import scipy.io.wavfile as wavfile
    import torch
    
    # åŠ è½½æ¨¡å‹
    processor = AutoProcessor.from_pretrained("suno/bark-small")
    model = BarkModel.from_pretrained("suno/bark-small")
    model.eval()
    
    print(f"[Task D] Model loaded, starting speech synthesis...")
    
    # å¤„ç†è¾“å…¥
    inputs = processor(text, voice_preset="v2/en_speaker_6")
    
    # ç”ŸæˆéŸ³é¢‘
    with torch.no_grad():
        audio_array = model.generate(**inputs)
    
    # è½¬æ¢ä¸º numpy æ•°ç»„
    audio_array = audio_array.cpu().numpy().squeeze()
    
    # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
    sample_rate = model.generation_config.sample_rate
    audio_file = os.path.join(OUTPUT_DIR, "generated_audio.wav")
    wavfile.write(audio_file, rate=sample_rate, data=audio_array)
    
    # è®¡ç®—æ—¶é•¿
    duration = len(audio_array) / sample_rate
    
    print(f"[Task D] âœ“ Audio generation completed!")
    print(f"[Task D]   Duration: {duration:.2f} seconds")
    print(f"[Task D]   Sample rate: {sample_rate} Hz")
    print(f"[Task D]   Saved to: {audio_file}")
    
    return {
        "audio_file_path": audio_file,
        "audio_duration": f"{duration:.2f}"
    }


# ============================================================================
# Task E: æ±‡æ€»å’Œå±•ç¤º
# ============================================================================
@task(
    inputs=["story_file", "image_file", "audio_file", "story_text", "image_info", "audio_duration"],
    outputs=["summary_html_path", "summary_text"],
    resources={"cpu": 1, "cpu_mem": 512, "gpu": 0, "gpu_mem": 0}
)
def summarize_results(params):
    """
    æ±‡æ€»æ‰€æœ‰ç”Ÿæˆçš„å†…å®¹ï¼Œåˆ›å»ºå±•ç¤ºé¡µé¢
    """
    story_file = params.get("story_file")
    image_file = params.get("image_file")
    audio_file = params.get("audio_file")
    story_text = params.get("story_text")
    image_info = params.get("image_info")
    audio_duration = params.get("audio_duration")
    
    print(f"[Task E] Aggregating all generated content...")
    
    # åˆ›å»º HTML å±•ç¤ºé¡µé¢
    html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>å¤šæ¨¡æ€åˆ›æ„å†…å®¹ç”Ÿæˆç»“æœ</title>
    <style>
        body {{
            font-family: Arial, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background: #f5f5f5;
        }}
        .container {{
            background: white;
            border-radius: 10px;
            padding: 30px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }}
        h1 {{
            color: #333;
            border-bottom: 3px solid #4CAF50;
            padding-bottom: 10px;
        }}
        .section {{
            margin: 30px 0;
            padding: 20px;
            background: #fafafa;
            border-radius: 8px;
        }}
        .section h2 {{
            color: #4CAF50;
            margin-top: 0;
        }}
        .story {{
            line-height: 1.8;
            color: #333;
        }}
        .image-container {{
            text-align: center;
            margin: 20px 0;
        }}
        .image-container img {{
            max-width: 100%;
            border-radius: 8px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.2);
        }}
        .info {{
            color: #666;
            font-size: 0.9em;
            margin-top: 10px;
        }}
        .badge {{
            display: inline-block;
            background: #4CAF50;
            color: white;
            padding: 5px 10px;
            border-radius: 5px;
            font-size: 0.85em;
            margin-right: 10px;
        }}
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ¨ å¤šæ¨¡æ€åˆ›æ„å†…å®¹ç”Ÿæˆç»“æœ</h1>
        <p style="color: #666;">é€šè¿‡ Maze åˆ†å¸ƒå¼å·¥ä½œæµå¹¶è¡Œç”Ÿæˆ</p>
        
        <div class="section">
            <h2>ğŸ“ ç”Ÿæˆçš„æ•…äº‹</h2>
            <div class="story">
                {story_text[:500]}...
            </div>
            <div class="info">
                <span class="badge">CPU</span>
                æ¨¡å‹: GPT-2 Large | å­—ç¬¦æ•°: {len(story_text)}
            </div>
        </div>
        
        <div class="section">
            <h2>ğŸ–¼ï¸ ç”Ÿæˆçš„å›¾åƒ</h2>
            <div class="image-container">
                <img src="{os.path.basename(image_file)}" alt="Generated Image">
            </div>
            <div class="info">
                <span class="badge">GPU</span>
                æ¨¡å‹: Stable Diffusion v1.5 | {image_info}
            </div>
        </div>
        
        <div class="section">
            <h2>ğŸ”Š ç”Ÿæˆçš„éŸ³é¢‘</h2>
            <audio controls style="width: 100%;">
                <source src="{os.path.basename(audio_file)}" type="audio/wav">
                æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒéŸ³é¢‘æ’­æ”¾
            </audio>
            <div class="info">
                <span class="badge">CPU</span>
                æ¨¡å‹: Bark TTS | æ—¶é•¿: {audio_duration} ç§’
            </div>
        </div>
        
        <div style="margin-top: 40px; padding: 20px; background: #e8f5e9; border-radius: 8px;">
            <h3 style="color: #2e7d32; margin-top: 0;">âš¡ æ€§èƒ½äº®ç‚¹</h3>
            <ul style="color: #333;">
                <li><strong>å¹¶è¡Œæ‰§è¡Œ:</strong> æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ç”ŸæˆåŒæ—¶è¿›è¡Œ</li>
                <li><strong>èµ„æºå¼‚æ„:</strong> CPU ä»»åŠ¡å’Œ GPU ä»»åŠ¡æ™ºèƒ½è°ƒåº¦</li>
                <li><strong>è‡ªåŠ¨ç®¡ç†:</strong> ä¾èµ–å…³ç³»è‡ªåŠ¨å¤„ç†ï¼Œç»“æœè‡ªåŠ¨æ±‡æ€»</li>
            </ul>
        </div>
    </div>
</body>
</html>
"""
    
    # ä¿å­˜ HTML
    html_file = os.path.join(OUTPUT_DIR, "result.html")
    with open(html_file, "w", encoding="utf-8") as f:
        f.write(html_content)
    
    # åˆ›å»ºæ–‡æœ¬æ‘˜è¦
    summary = f"""
{'='*70}
å¤šæ¨¡æ€åˆ›æ„å†…å®¹ç”Ÿæˆå®Œæˆï¼
{'='*70}

ğŸ“ è¾“å‡ºæ–‡ä»¶:
  - æ•…äº‹æ–‡æœ¬: {story_file}
  - ç”Ÿæˆå›¾åƒ: {image_file}
  - åˆæˆéŸ³é¢‘: {audio_file}
  - æ±‡æ€»é¡µé¢: {html_file}

ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:
  - æ•…äº‹é•¿åº¦: {len(story_text)} å­—ç¬¦
  - å›¾åƒä¿¡æ¯: {image_info}
  - éŸ³é¢‘æ—¶é•¿: {audio_duration} ç§’

ğŸŒ æŸ¥çœ‹ç»“æœ:
  åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€: file:///{html_file}

âœ¨ å·¥ä½œæµç‰¹ç‚¹:
  - 3 ä¸ªä»»åŠ¡å¹¶è¡Œæ‰§è¡Œ (B, C, D)
  - CPU å’Œ GPU èµ„æºå¼‚æ„è°ƒåº¦
  - è‡ªåŠ¨ä¾èµ–ç®¡ç†å’Œç»“æœæ±‡æ€»
{'='*70}
"""
    
    print(f"[Task E] âœ“ Aggregation completed!")
    print(summary)
    
    return {
        "summary_html_path": html_file,
        "summary_text": summary
    }


# ============================================================================
# ä¸»ç¨‹åºï¼šç¼–æ’å·¥ä½œæµ
# ============================================================================
def main():
    print("=" * 70)
    print("ğŸ¨ Multimodal Creative Content Generator")
    print("=" * 70)
    print()
    
    # User input
    user_input = input("Enter your creative theme (e.g., a magical forest at sunset): ").strip()
    if not user_input:
        user_input = "a magical forest at sunset with glowing fireflies"
        print(f"Using default theme: {user_input}")
    
    print()
    print("ğŸš€ Starting workflow creation...")
    print()
    
    # 1. åˆ›å»ºå®¢æˆ·ç«¯
    client = MaClient("http://localhost:8000")
    
    # 2. Create workflow
    workflow = client.create_workflow()
    print(f"âœ“ Workflow created: {workflow.workflow_id}")
    
    # 3. Add Task A: Preprocessing
    print("âœ“ Adding Task A: Text preprocessing and prompt enhancement")
    task_a = workflow.add_task(
        preprocess_and_enhance,
        inputs={"user_description": user_input},
        task_name="Preprocessing and Prompt Enhancement"
    )
    
    # 4. Add parallel tasks B, C, D
    print("âœ“ Adding Task B: Text story generation (CPU)")
    task_b = workflow.add_task(
        generate_story,
        inputs={"text_prompt": task_a.outputs["enhanced_text_prompt"]},
        task_name="Text Story Generation"
    )
    
    print("âœ“ Adding Task C: Image generation (GPU)")
    task_c = workflow.add_task(
        generate_image,
        inputs={"image_prompt": task_a.outputs["enhanced_image_prompt"]},
        task_name="Image Generation"
    )
    
    print("âœ“ Adding Task D: Audio generation (CPU)")
    task_d = workflow.add_task(
        generate_audio,
        inputs={"audio_text": task_a.outputs["audio_text"]},
        task_name="Audio Generation"
    )
    
    # 5. Add Task E: Aggregation
    print("âœ“ Adding Task E: Result aggregation")
    task_e = workflow.add_task(
        summarize_results,
        inputs={
            "story_file": task_b.outputs["story_file_path"],
            "image_file": task_c.outputs["image_file_path"],
            "audio_file": task_d.outputs["audio_file_path"],
            "story_text": task_b.outputs["generated_story"],
            "image_info": task_c.outputs["image_info"],
            "audio_duration": task_d.outputs["audio_duration"]
        },
        task_name="Result Aggregation"
    )
    
    print()
    print("ğŸ“Š Workflow structure:")
    print("    Task A (Preprocessing)")
    print("       â†“")
    print("    â”Œâ”€â”€â”´â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”")
    print("    â†“     â†“      â†“")
    print("  Task B Task C Task D  â† Parallel execution")
    print("  (CPU)  (GPU)  (CPU)")
    print("    â””â”€â”€â”¬â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜")
    print("       â†“")
    print("    Task E (Aggregation)")
    print()
    
    # 6. Run workflow
    print("ğŸš€ Starting workflow execution...")
    print("=" * 70)
    print()
    
    workflow.run()
    
    # 7. Get results with automatic progress display
    result = workflow.show_results(output_dir=OUTPUT_DIR)
    
    print()
    print("ğŸ“ All files saved to:", OUTPUT_DIR)
    print("ğŸŒ Open result.html to view complete results")
    print()


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  User interrupted execution")
    except Exception as e:
        print(f"\n\nâŒ Execution error: {e}")
        import traceback
        traceback.print_exc()


